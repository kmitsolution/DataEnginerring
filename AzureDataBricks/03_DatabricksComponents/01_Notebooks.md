# ğŸš€ Databricks Notebook Best Practices (Beginner â†’ Advanced)

Applies to:
Azure Databricks with **Unity Catalog enabled**

This guide is structured for GitHub notes and interview preparation.

---

# ğŸ”· 1ï¸âƒ£ Workspace & Folder Structure (Start Clean)

Before writing code, organize properly.

## âœ… Recommended Folder Structure

```
Workspace/
 â”œâ”€â”€ Shared/
 â”‚    â”œâ”€â”€ utils/
 â”‚    â”œâ”€â”€ configs/
 â”‚
 â”œâ”€â”€ Bronze/
 â”œâ”€â”€ Silver/
 â”œâ”€â”€ Gold/
 â”‚
 â”œâ”€â”€ Jobs/
 â””â”€â”€ Experiments/
```

### Why?

* Separation of concerns
* Easier collaboration
* Cleaner production deployment

---

# ğŸ”· 2ï¸âƒ£ Use Clear Notebook Naming Convention

âŒ Bad:

```
test1
new notebook
abc
```

âœ… Good:

```
01_bronze_ingestion_sales
02_silver_clean_sales
03_gold_sales_aggregation
```

Benefits:

* Execution order clear
* Easy orchestration
* Professional structure

---

# ğŸ”· 3ï¸âƒ£ Start Every Notebook with Markdown Documentation

Use `%md` for clarity.

```markdown
%md
# Bronze Layer - Sales Ingestion

## Source:
- ADLS container: raw
- Format: CSV

## Target:
- Unity Catalog Table: main.bronze.sales_raw
```

Why?

* Self-documenting code
* Easy onboarding
* Interview-friendly

---

# ğŸ”· 4ï¸âƒ£ Use Parameters (Avoid Hardcoding)

âŒ Bad:

```python
path = "abfss://raw@storage.dfs.core.windows.net/sales/"
```

âœ… Good (Widgets):

```python
dbutils.widgets.text("env", "dev")
env = dbutils.widgets.get("env")
```

Use:

* For dev/test/prod
* For reusability
* For Jobs

---

# ğŸ”· 5ï¸âƒ£ Avoid Using `/mnt` (Modern Approach)

With Unity Catalog:

âŒ Avoid:

```
/mnt/datalake
```

âœ… Use:

```
/Volumes/catalog/schema/volume
```

or

```
abfss://container@storageaccount.dfs.core.windows.net/
```

---

# ğŸ”· 6ï¸âƒ£ Always Use Explicit Schema

âŒ Bad:

```python
df = spark.read.csv(path, header=True)
```

âœ… Good:

```python
from pyspark.sql.types import *

schema = StructType([
    StructField("id", IntegerType(), True),
    StructField("amount", DoubleType(), True)
])

df = spark.read.schema(schema).csv(path, header=True)
```

Why?

* Better performance
* Avoid schema inference errors
* Production-ready

---

# ğŸ”· 7ï¸âƒ£ Use Delta Format (Always)

âŒ Donâ€™t write parquet directly in production.

âœ… Use Delta:

```python
df.write.format("delta") \
  .mode("overwrite") \
  .saveAsTable("main.bronze.sales_raw")
```

Why?

* ACID
* Time travel
* Schema enforcement

---

# ğŸ”· 8ï¸âƒ£ Follow Medallion Architecture

Notebook separation:

```
Bronze â†’ Raw ingestion
Silver â†’ Cleaning & joins
Gold â†’ Aggregations & KPIs
```

Never mix logic of multiple layers in one notebook.

---

# ğŸ”· 9ï¸âƒ£ Error Handling (Important for Jobs)

Use try-except:

```python
try:
    df.write.format("delta").mode("append").saveAsTable("main.bronze.sales_raw")
except Exception as e:
    print("Error occurred:", e)
    raise
```

For production:

* Log properly
* Fail fast

---

# ğŸ”· ğŸ”Ÿ Use Logging Instead of print()

Better approach:

```python
import logging

logger = logging.getLogger("sales_job")
logger.setLevel(logging.INFO)

logger.info("Ingestion started")
```

Why?

* Professional
* Production ready

---

# ğŸ”· 1ï¸âƒ£1ï¸âƒ£ Avoid Large Collect()

âŒ Dangerous:

```python
df.collect()
```

Brings full dataset to driver memory.

âœ… Use:

```python
df.show(10)
```

or

```python
df.limit(10).display()
```

---

# ğŸ”· 1ï¸âƒ£2ï¸âƒ£ Optimize Writes

Use partitioning carefully:

```python
df.write.format("delta") \
  .partitionBy("date") \
  .mode("append") \
  .saveAsTable("main.silver.sales_clean")
```

Avoid:

* Over-partitioning
* Too many small files

---

# ğŸ”· 1ï¸âƒ£3ï¸âƒ£ Use Reusable Utility Notebooks

Create:

```
/Shared/utils/common_functions
```

Call using:

```python
%run /Workspace/Shared/utils/common_functions
```

Good for:

* Validation functions
* Logging utilities
* Common transformations

---

# ğŸ”· 1ï¸âƒ£4ï¸âƒ£ Use Jobs Instead of Manual Runs

Development:

* Interactive cluster

Production:

* Job cluster
* Scheduled workflow

Never run production pipelines manually.

---

# ğŸ”· 1ï¸âƒ£5ï¸âƒ£ Cluster Best Practices

For development:

* Small cluster
* Auto terminate 30â€“60 mins

For production:

* Job cluster
* Autoscaling enabled
* Photon enabled

---

# ğŸ”· 1ï¸âƒ£6ï¸âƒ£ Code Cleanliness

Follow:

* PEP8 for Python
* Proper indentation
* No unnecessary commented code
* Modular functions

---

# ğŸ”· 1ï¸âƒ£7ï¸âƒ£ Security Best Practices

âœ… Use:

* Unity Catalog
* Role-based access
* Managed identity
* Secret scopes

âŒ Avoid:

* Hardcoding credentials
* Public DBFS root
* Exposing keys

---

# ğŸ”· 1ï¸âƒ£8ï¸âƒ£ Version Control (Very Important)

Never rely only on workspace.

Integrate with:

* Azure DevOps
* GitHub

Use:

* Repos feature in Databricks
* Branching strategy

---

# ğŸ”· 1ï¸âƒ£9ï¸âƒ£ Performance Best Practices

* Use broadcast joins for small tables
* Avoid unnecessary shuffle
* Cache only when needed
* Use OPTIMIZE on Delta tables
* Enable Photon

---

# ğŸ”· 2ï¸âƒ£0ï¸âƒ£ Production Notebook Checklist

Before deploying:

âœ” Uses Unity Catalog
âœ” No hardcoded paths
âœ” Explicit schema
âœ” Delta format
âœ” Error handling
âœ” Parameterized
âœ” Logging added
âœ” Tested on small data

---

# ğŸ”¥ Common Beginner Mistakes

| Mistake                    | Why Bad          |
| -------------------------- | ---------------- |
| Using /mnt                 | Deprecated       |
| Using inferSchema          | Slow             |
| Using collect()            | OOM risk         |
| Mixing Bronze & Gold logic | Poor design      |
| No documentation           | Hard to maintain |

---

# ğŸ¯ Final Professional Structure (Example)

```
01_bronze_sales_ingestion
02_silver_sales_cleaning
03_gold_sales_kpi
04_job_orchestration
```

Each notebook:

* Markdown header
* Parameters
* Explicit schema
* Delta write
* Logging
* Error handling

---

#  Interview Summary Answer

Databricks notebook best practices include:

* Clear folder structure
* Medallion architecture separation
* Use Delta format
* Use Unity Catalog
* Parameterization
* Logging and error handling
* Avoid collect()
* Version control integration
* Production-ready job orchestration

---


Tell me what you want next ğŸš€
