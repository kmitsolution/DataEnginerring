
#  Databricks Magic Commands (Updated for Unity Catalog)

Works with:

* Unity Catalog enabled workspace
* No public DBFS root
* Volumes
* External locations

---

# ğŸ”· 1ï¸âƒ£ %sql (Unity Catalog Aware)

Use `%sql` to work with:

* Catalog
* Schema
* Tables
* Volumes
* External locations

---

## ğŸ”¹ Show Catalogs

```sql
%sql
SHOW CATALOGS;
```

---

## ğŸ”¹ Use Catalog

```sql
%sql
USE CATALOG main;
```

---

## ğŸ”¹ Show Schemas

```sql
%sql
SHOW SCHEMAS;
```

---

## ğŸ”¹ Use Schema

```sql
%sql
USE SCHEMA default;
```

---

## ğŸ”¹ Create Table in Unity Catalog

```sql
%sql
CREATE TABLE sales_data (
  id INT,
  amount DOUBLE
);
```

---

## ğŸ”¹ Select Data

```sql
%sql
SELECT * FROM sales_data;
```

---

# ğŸ”· 2ï¸âƒ£ %fs (With Unity Catalog Volumes)

âš ï¸ Important: `/mnt` may not work anymore.

Instead use:

```
/Volumes/<catalog>/<schema>/<volume>/
```

---

## ğŸ”¹ List Volume Files

```python
%fs ls /Volumes/main/default/raw_data
```

---

## ğŸ”¹ Create Folder in Volume

```python
%fs mkdirs /Volumes/main/default/raw_data/new_folder
```

---

## ğŸ”¹ Remove Folder

```python
%fs rm /Volumes/main/default/raw_data/new_folder -r
```

---

# ğŸ”· 3ï¸âƒ£ %pip (Same as Before)

Still works the same.

## ğŸ”¹ Install Package

```python
%pip install pandas
```

## ğŸ”¹ Restart Python (If Needed)

```python
dbutils.library.restartPython()
```

---

# ğŸ”· 4ï¸âƒ£ %run (Reusable Notebooks)

Still used for modular notebook design.

Example:

```python
%run /Workspace/Shared/utils/common_functions
```

âš ï¸ Path format may vary depending on workspace folder.

---

# ğŸ”· 5ï¸âƒ£ Accessing Data Using ABFSS (Recommended Instead of Mount)

Instead of:

```
/mnt/datalake
```

Use:

```
abfss://container@storageaccount.dfs.core.windows.net/
```

---

## ğŸ”¹ Example: Read Delta Table

```python
df = spark.read.format("delta").load(
  "abfss://container@storageaccount.dfs.core.windows.net/bronze/sales"
)
display(df)
```

---

# ğŸ”· 6ï¸âƒ£ Unity Catalog Volume Write Example

## ğŸ”¹ Write CSV to Volume

```python
df.write.mode("overwrite").csv(
  "/Volumes/main/default/raw_data/sales_csv"
)
```

---

## ğŸ”¹ Read from Volume

```python
df = spark.read.csv(
  "/Volumes/main/default/raw_data/sales_csv",
  header=True
)
display(df)
```

---

# ğŸ”· 7ï¸âƒ£ Create Volume (Unity Catalog SQL)

```sql
%sql
CREATE VOLUME raw_data;
```

---

# ğŸ”· 8ï¸âƒ£ Create External Location (Admin Use)

```sql
%sql
CREATE EXTERNAL LOCATION my_external_loc
URL 'abfss://container@storageaccount.dfs.core.windows.net/'
WITH (STORAGE CREDENTIAL my_credential);
```

---

# ğŸ”· 9ï¸âƒ£ Important dbutils Commands (Still Valid)

## ğŸ”¹ List Files

```python
dbutils.fs.ls("/Volumes/main/default/raw_data")
```

---

## ğŸ”¹ Get Notebook Context

```python
dbutils.notebook.entry_point.getDbutils().notebook().getContext()
```

---

# ğŸ”· Old vs New (Important for Interview)

| Old Way                  | New Unity Catalog Way  |
| ------------------------ | ---------------------- |
| /mnt                     | /Volumes               |
| DBFS root                | Unity Catalog volumes  |
| Mount storage            | External location      |
| No fine-grained security | Catalog-based security |

---

# ğŸ”¥ Real Production Flow (Modern Databricks)

```text
1. USE CATALOG main
2. USE SCHEMA bronze
3. Read from ABFSS
4. Write to Delta Table
5. Store files in Volume
6. Govern access using Unity Catalog
```

---

# ğŸ”· Most Important Interview Points

### Q: Why avoid /mnt now?

Because public DBFS root is disabled in Unity Catalog-enabled workspaces.

### Q: What replaces mounts?

External Locations + Volumes.

### Q: How do you access storage now?

Using:

* abfss path
* Unity Catalog volumes

---

# ğŸ¯ Final Best Practice

For new projects:

âœ… Use Unity Catalog
âœ… Use Volumes
âœ… Use ABFSS direct paths
âŒ Avoid legacy DBFS root

---

